{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29601c70",
   "metadata": {},
   "source": [
    "Goal: Implement a decision tree from scratch and apply it to the task of classifying whether a mushroom is edible or poisonous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d07d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from public_tests import *\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8a3b5",
   "metadata": {},
   "source": [
    "We are playing the role of a company that grows and sells wild mushrooms. Nevermind the linguistics, apparently.\n",
    "\n",
    "Anyway, poisonous mushrooms are a real concern, see? And we have some existing data to help us \n",
    "determind, based on physical traits, which ones are.\n",
    "\n",
    "The dataset itself has $10$ examples with $3$ features ($x$):\n",
    "\n",
    "\n",
    "1. Cap color (Browm, red)\n",
    "\n",
    "2. Stalk shape (Tapering, enlargening)\n",
    "\n",
    "3. Solitary (Yes, no)\n",
    "\n",
    "And the output, ($y$):\n",
    "\n",
    "Edible (Y/N)\n",
    "\n",
    "\n",
    "For ease, we can one-hot encode these features to be:\n",
    "\n",
    "1. Brown cap?\n",
    "\n",
    "2. Tapering stalk shape?\n",
    "\n",
    "3. Solitary\n",
    "\n",
    "4. Edible\n",
    "\n",
    "So a single example may look like:\n",
    "\n",
    "$0101$\n",
    "\n",
    "Indicating: Red cap, tapering, clustered, edible mushroom.\n",
    "\n",
    "\n",
    "Time to load it in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c060eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 1, 1], [1, 0, 1], [1, 0, 0], [1, 0, 0], [1, 1, 1],\n",
    "                   [0, 1, 1], [0, 0, 0], [1, 0, 1], [0, 1, 0], [1, 0, 0]])\n",
    "y_train = np.array([1, 1, 0, 0, 1, 0, 0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "166ac519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 examples from X_train: \n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "Type of X_train: <class 'numpy.ndarray'>\n",
      "\n",
      "First 5 results from y_train: [1 1 0 0 1]\n",
      "Type of y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# In a more complicated case, we may want to get a brief view of the data and the data type(s)\n",
    "print(f\"First 5 examples from X_train: \\n{X_train[:5]}\")\n",
    "print(f\"Type of X_train: {type(X_train)}\")\n",
    "print(f\"\\nFirst 5 results from y_train: {y_train[:5]}\")\n",
    "print(f\"Type of y_train: {type(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50f8ead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train's shape:\t\t\t(10, 3)\n",
      "y_train's shape:\t\t\t(10,)\n",
      "Number of training examples (m):\t10\n"
     ]
    }
   ],
   "source": [
    "# Another preliminary step is to check the dimensions of the data:\n",
    "print(f\"X_train's shape:\\t\\t\\t{X_train.shape}\")\n",
    "print(f\"y_train's shape:\\t\\t\\t{y_train.shape}\")\n",
    "print(f\"Number of training examples (m):\\t{len(X_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92140d",
   "metadata": {},
   "source": [
    "As a refresher, building a decision tree has the following components:\n",
    "\n",
    "1. Start with all examples in the root node.\n",
    "\n",
    "2. Calculate information gain for splitting on all possible features ($3$), pick the feature with the highest information gain.\n",
    "\n",
    "3. Split the dataset according to the selected feature, creating left and right branches of the tree.\n",
    "\n",
    "4. Repeat this process until a stopping criterion is met.\n",
    "\n",
    "$ $\n",
    "\n",
    "To do these things, we need a couple of functions:\n",
    "- Calculate node entropy\n",
    "- Split dataset at a node into left/right branches based on a feature.\n",
    "- Calculate information gain from splitting on a given feature.\n",
    "- Choose feature that maximizes information gain.\n",
    "\n",
    "(Plus some helper functions to repeat split until a criterion is met, in this case just a max depth of $2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614ee17",
   "metadata": {},
   "source": [
    "Recall that entropy is calculated using $p_1$, the fraction of examples with $y=1$ of a subset as:\n",
    "\n",
    "$$H(p_1) = -p_1log_2(p_1) - (1 - p_1)log_2(1 - p_1)$$\n",
    "\n",
    "And for implementation purposes, $log_2(0) = 0$\n",
    "\n",
    "Also remember to check the data at a node is not empty. Since our only criterion is depth $= 2$\n",
    "\n",
    "If it is, return $0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14392e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for a subset of data.\n",
    "    \n",
    "    Args:\n",
    "        y (ndarray): Numpy array indicating whether each example at a node is edible (1) or not (0)\n",
    "        \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(y)\n",
    "    if m == 0:\n",
    "        return 0\n",
    "    \n",
    "    p1 = np.sum(y) / m\n",
    "    if p1 == 0 or p1 == 1:\n",
    "        return 0\n",
    "    \n",
    "    return -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa016dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy at root: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 10 examples, 50/50 => Expect entropy = 1.0\n",
    "print(f\"Entropy at root: {compute_entropy(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d0a3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into left and right branches.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list): List containing the active indices\n",
    "        feature (int): Index of feature to split on\n",
    "        \n",
    "    Returns:\n",
    "        left_indices (list): Indices with feature value == 1\n",
    "        right_indices (list): Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    \n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    \n",
    "    for i in node_indices:\n",
    "        if X[i][feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "            \n",
    "    return left_indices, right_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd234af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 7, 9] [5, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "root_indices = [x for x in range(10)]\n",
    "feature = 0 # Cap color, brown = 1, red = 0\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "print(left_indices, right_indices) # Expect right = [5, 6, 8], left is remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc10115",
   "metadata": {},
   "source": [
    "Now we can split and calculate entropy, need to calculate information gain.\n",
    "\n",
    "Recall that:\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^{\\text{node}}) - (w^{\\text{left}} H(p_1^{\\text{left}}) + w^{\\text{right}} H(p_1^{\\text{right}}))$$\n",
    "\n",
    "Where $w$ is the proportion of examples in each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68dddcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Computes the information gain from splitting a node on a given feature.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Data matric of shape(n_samples, n_features)\n",
    "        y (array like): list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices\n",
    "        feature (int): Index of feature being split on\n",
    "        \n",
    "    Returns:\n",
    "        information_gain (float): Information gain computer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initially, split the dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Helper variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    # Compute information gain\n",
    "    entropy_node = compute_entropy(y_node)\n",
    "    entropy_left = compute_entropy(y_left)\n",
    "    entropy_right = compute_entropy(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    \n",
    "    information_gain = entropy_node - (w_left * entropy_left + w_right * entropy_right)\n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dc2f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gains = [compute_information_gain(X_train, \n",
    "                                       y_train,\n",
    "                                       [x for x in range(X_train.shape[0])],\n",
    "                                       i)\n",
    "              for i in range(X_train.shape[1])\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "689312e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.034851554559677034, 0.12451124978365313, 0.2780719051126377]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gains # Max is on feature 2, is the sample is solitary or not. But let's automate this with a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee3347bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(X, y, node_indices):\n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value to split the node data on.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray): Data matrix of shape(n_samples, n_features)\n",
    "      y (array like): list or ndarray with n_samples containing the target variable\n",
    "      node_indices (ndarray): List containing the active indices\n",
    "      \n",
    "    Returns:\n",
    "        best_feature (int): The index of the best feature to split on\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    gains_per_feature = [compute_information_gain(X, y, node_indices, i) for i in range(n_features)]\n",
    "    \n",
    "    best_feature = np.argmax(gains_per_feature)\n",
    "    \n",
    "    if gains_per_feature[best_feature] == 0:\n",
    "        return -1 # Pure set, no split is best...but really should be handled by criterion.\n",
    "    \n",
    "    return best_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fdf5a46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_split(X_train, y_train, root_indices) # Expect 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15166932",
   "metadata": {},
   "source": [
    "Now, actually put this all together to build the tree.\n",
    "We only have the one criterion, max depth of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad1a3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = []\n",
    "\n",
    "def build_tree_recursion(X, y, node_indices, branch_name, curr_depth=0, max_depth=2):\n",
    "    \"\"\"\n",
    "    Builds a tree using a recursive algorithm that splits the dataset into 2 subgroups at each node.\n",
    "    Prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Data matrix of shape(n_samples, n_features)\n",
    "        y (array like): list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices\n",
    "        branch_name (string): Name of branch (Root, Left, or Right)\n",
    "        max_depth (int): Max depth of resulting tree\n",
    "        curr_depth (int): Current depth, used during recursion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base case - max depth reached\n",
    "    if curr_depth == max_depth:\n",
    "        formatting = \" \" * curr_depth + \"-\" * curr_depth\n",
    "        print(f\"{formatting} {branch_name} node with indices {node_indices}\")\n",
    "        return\n",
    "    \n",
    "    # If not base/final case, get best split, then act on it\n",
    "    best_feat = get_best_split(X, y, node_indices)\n",
    "    formatting = \"-\" * curr_depth\n",
    "    print(f\"{formatting} Depth {curr_depth}, {branch_name}: Split on feature {best_feat}\")\n",
    "    \n",
    "    left, right = split_dataset(X, node_indices, best_feat)\n",
    "    tree.append((left, right, best_feat))\n",
    "    \n",
    "    # Keep the recursion going\n",
    "    build_tree_recursion(X, y, left, \"Left\", curr_depth + 1)\n",
    "    build_tree_recursion(X, y, right, \"Right\", curr_depth + 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78ba3a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature 2\n",
      "- Depth 1, Left: Split on feature 0\n",
      "  -- Left node with indices [0, 1, 4, 7]\n",
      "  -- Right node with indices [5]\n",
      "- Depth 1, Right: Split on feature 1\n",
      "  -- Left node with indices [8]\n",
      "  -- Right node with indices [2, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "build_tree_recursion(X_train, y_train, root_indices, \"Root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbef6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
