{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacb7536",
   "metadata": {},
   "source": [
    "This lab is to visualize using information gain to split a decision tree node.\n",
    "Dataset involves determining if an image is of a cat or dog, using features:\n",
    "\n",
    "Ear shape: Pointy = 1; Floppy = 0;\n",
    "\n",
    "Face shape: Round = 1; Not Round = 0;\n",
    "\n",
    "Whiskers: Present = 1; Absent = 0;\n",
    "\n",
    "The output is a binary digit: Cat = 1; Dog = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3f5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d34acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X_train = np.array([\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27ce203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # Pointy ears, round face, and whiskers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9d62e",
   "metadata": {},
   "source": [
    "From the notes:\n",
    "\n",
    "$\\text{Information Gain} = H(p_1^{node}) - (w^{left} * H(p_1^{left}) + w^{right} * H(p_1^{right}))$\n",
    "\n",
    "Where $H$ is the entropy, defined as:\n",
    "\n",
    "$H(p_1) = -p_1 * log_2(p_1) - (1 - p_1) * log_2(1 - p_1)$\n",
    "\n",
    "$ $\n",
    "\n",
    "Note log is in base 2.\n",
    "\n",
    "$ $\n",
    "\n",
    "On each node, compute the information gain for each feature, splitting the node on the feature with the highest information gain, by comparing the entropy of the node with the **weighted** entropy of the two split nodes.\n",
    "\n",
    "$ $\n",
    "\n",
    "Keeping in mind that the whole dataset has 5 cats and 5 dogs in it, the root node which has all these classes results in:\n",
    "\n",
    "$p_1^{node} = \\frac{5}{10} = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea78d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    # Compute entropy of a group based on the percentage of positive cases in a group.\n",
    "    if p == 0 or p == 1:\n",
    "        return 0 # Pure set either way, no entropy.\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1 - p) * np.log2(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe54e642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(entropy(0.5)) # The least pure possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9750d0",
   "metadata": {},
   "source": [
    "So now compute the information gain if we split the initial node on each of the available features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65f0f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    \"\"\"Given a dataset, X, and an index feature, return two lists for the split nodes.\n",
    "    The left node is the positive case, with the feature equal to 1.\n",
    "    The right node is the negative case, with the feature equal to 0.\n",
    "    From above:\n",
    "    index_feature 0 -> ear shape\n",
    "    index_feature 1 -> face shape\n",
    "    index_feature 2 -> whiskers\n",
    "    \"\"\"\n",
    "    \n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i, x in enumerate(X):\n",
    "        if x[index_feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b0acc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 3, 4, 5, 7], [1, 2, 6, 8, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_indices(X_train, 0) # Ear shape pointy, ear shape floppy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0f47e",
   "metadata": {},
   "source": [
    "We also need to compute the weighted entropy in a split node, needing $w^{left}$ and $w^{right}$ (proportions of examples in each node) as well as $p^{left}$ and $p^{right}$ (proportions of positive examples in each split).\n",
    "\n",
    "$ $\n",
    "\n",
    "So for example, using the above results, \n",
    "\n",
    "$w^{left} = \\frac{5 \\text{(length of left array)}}{10 \\text{(length of parent node)}}$ and \n",
    "\n",
    "$p^{left} = \\frac{4 \\text{(number in left array that are positive)}}{5 \\text{(length of array)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f824918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(X, y, left_indices, right_indices):\n",
    "    \"\"\"Given a dataset and the split dataset on some feature, \n",
    "    return the weighted entropy of that split.\"\"\"\n",
    "    w_left = len(left_indices) / len(X)\n",
    "    w_right = len(right_indices) / len(X)\n",
    "    # Don't really need X for this, huh?\n",
    "    p_left = sum(y[left_indices]) / len(left_indices)\n",
    "    p_right = sum(y[right_indices]) / len(right_indices)\n",
    "    \n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "179a000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7219280948873623\n"
     ]
    }
   ],
   "source": [
    "left_indices, right_indices = split_indices(X_train, 0)\n",
    "WE = weighted_entropy(X_train, y_train, left_indices, right_indices)\n",
    "print(WE) # Expect ~0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfcc5db",
   "metadata": {},
   "source": [
    "So from the weighted entropy, the information gain is got by subtracting from entropy of parent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04a6d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, left_indices, right_indices):\n",
    "    p_node = sum(y) / len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X, y, left_indices, right_indices)\n",
    "    \n",
    "    return h_node - w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a340b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2780719051126377"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain(X_train, y_train, left_indices, right_indices) # Expect ~0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff7dc4",
   "metadata": {},
   "source": [
    "Then do the same for each other feature to find the highesst information gain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ac6e00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: Ear Shape\n",
      "Information Gain on split: 0.28\n",
      "\n",
      "Feature: Face Shape\n",
      "Information Gain on split: 0.03\n",
      "\n",
      "Feature: Whiskers\n",
      "Information Gain on split: 0.12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, feature_name in enumerate(['Ear Shape', 'Face Shape', 'Whiskers']):\n",
    "    left_indices, right_indices = split_indices(X_train, i)\n",
    "    i_gain = information_gain(X_train, y_train, left_indices, right_indices)\n",
    "    print(f\"Feature: {feature_name}\\nInformation Gain on split: {i_gain:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516366c",
   "metadata": {},
   "source": [
    "Based on this, ear shape is the best feature to split  on, as we gain the most information from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2637e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c87d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
